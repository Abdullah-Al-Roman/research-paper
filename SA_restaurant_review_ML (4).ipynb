{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123ranika/Research-paper/blob/main/SA_restaurant_review_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r6TMW419g-c"
      },
      "source": [
        "# Project: Sentiment Analysis on Bengali Restaurant Reviews\n",
        "\n",
        "In this project we will classify the sentiment of a review either it is positive or negative. For this we have created a dataset of $1.4k$ bengali restaurant reviews. It is a balanced dataset where $630$ reviews are annotated as Positive Sentiment and another $790$ reviews as negative sentiment. All the Reviews are collected from different social media groups( such as food monster) and then manually annotated by two native bengali speaker.  \n",
        "\n",
        "\n",
        "**Project Includes:**\n",
        "\n",
        "-   Preprocessing\n",
        "-   Exploratory Analysis\n",
        "-   Feature Extraction using TF-IDF for N-gram\n",
        "-   Machine Learning Model Development\n",
        "-   Evaluation Measure\n",
        "-   Saved the Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqkc2fT89g-f"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "Df1J-owJ9g-g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import re,json,nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
        "stopwords_list ='stopwords-bn.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDL337Ji9g-h"
      },
      "source": [
        "## Importing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBxMLrKp9g-h",
        "outputId": "c6237aa2-9d13-4c9a-8bd6-ae1997cb519e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reviews: 6000 \n",
            "Total Cyberbullying Reviews: 1686 \n",
            "Total Religious_Hatred Reviews: 673 \n",
            "Total Gender_Discrimination Reviews: 678 \n",
            "Total Sarcasm Reviews: 1518 \n",
            "Total Political Reviews: 776 \n",
            "Total Racism Reviews: 668\n"
          ]
        }
      ],
      "source": [
        "# Read the data and take only 1000 Reviews\n",
        "data = pd.read_csv('/content/Traning.xlsx - Sheet1 (6).csv',encoding='UTF-8')\n",
        "print(\"Total Reviews:\",len(data),\n",
        "      \"\\nTotal Cyberbullying Reviews:\",len(data[data.labels =='Cyberbullying']),\n",
        "      \"\\nTotal Religious_Hatred Reviews:\",len(data[data.labels=='Religious_Hatred']),\n",
        "      \"\\nTotal Gender_Discrimination Reviews:\",len(data[data.labels =='Gender_Discrimination']),\n",
        "      \"\\nTotal Sarcasm Reviews:\",len(data[data.labels=='Sarcasm']),\n",
        "      \"\\nTotal Political Reviews:\",len(data[data.labels =='Political']),\n",
        "      \"\\nTotal Racism Reviews:\",len(data[data.labels=='Racism'])\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPFycGqF9g-j",
        "outputId": "44139ea1-bfe9-4a62-a397-8c1605cb5fc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['PID', 'text', 'labels'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqPWPydl9g-l",
        "outputId": "af6ea7cc-1e6d-43a2-8e9b-58d91c4a4d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "আল্লাহ এদের উপর গজব নাজিল করুক। \n",
            " labels:--  Religious_Hatred \n",
            "\n",
            "ধুত তেরি শালার পুতেরা \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "চাপা দিয়ে নিজেকে দিয়েছি ধিক্কার,, \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "হ্স্ত মেরে করবো শেষ \n",
            " labels:--  Sarcasm \n",
            "\n",
            "মানগের নাতি, ইজলা এতে কিয়া কয় \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "৯ বার ব্রাজিলের পোদ মেরে জেতা \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "হিন্দুর সোনা মজা \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "তাদের যৌন সন্তুষ্টি \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "ওরে কানাচোদা,,,তোদের ওই লেওড়া খান ওরফে ছাকিব খান \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "নাটকের রুনা খানের মতো লাগে এখন \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "হিন্দুর সোনা মজা \n",
            " labels:--  Cyberbullying \n",
            "\n",
            "ভারতের রেসিজম টা চরমে! \n",
            " labels:--  Racism \n",
            "\n",
            "কই সোনা \n",
            " labels:--  Sarcasm \n",
            "\n",
            "মাইয়ারে পাইলে দুধ এর উপর ঘুম যাইতাম আহা সেই মাল \n",
            " labels:--  Gender_Discrimination \n",
            "\n",
            "আপনি নিজে ভালো তো ভাই \n",
            " labels:--  Sarcasm \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print some unprocessed reviews\n",
        "sample_data = [10,100,150,200,250,600,650,666,689,640,650,700,750,800,1000]\n",
        "for i in sample_data:\n",
        "      print(data.text[i],'\\n','labels:-- ',data.labels[i],'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBXvI0A39g-l"
      },
      "source": [
        "## Data Processing\n",
        "This step includes removal of punctuation mark, numbers, emoji and stopwords from the reviews. We have used a helper functions for cleaning the corpus.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the contents of the utils module\n",
        "import utils\n",
        "\n",
        "# This will list all attributes and functions in the utils module\n",
        "print(dir(utils))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpdvRD7CDbBM",
        "outputId": "7f3a942d-8275-4511-f38c-4ea080716fff"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "spec = importlib.util.spec_from_file_location(\"utils\", \"/content/utils.py\")\n",
        "utils = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(utils)\n",
        "\n",
        "# Now you can access the functions\n",
        "cleaned_reviews = utils.cleaned_reviews\n",
        "stopwords_info = utils.stopwords_info\n",
        "stopword_removal = utils.stopword_removal\n",
        "process_reviews = utils.process_reviews\n"
      ],
      "metadata": {
        "id": "aR7s2BVaDhl7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "source": [
        "#from utils import cleaned_reviews,stopwords_info,stopword_removal,process_reviews\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jdRZvDzBAR0M"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TzB2kk2o9g-n"
      },
      "outputs": [],
      "source": [
        "def process_reviews(review, stopwords, removing_stopwords=True):\n",
        "    \"\"\"This function takes a review string as input and returns a cleaned review string\n",
        "        after removing punctuation, English characters, numbers and stopwords.\n",
        "\n",
        "        Args:\n",
        "            review: str\n",
        "            stopwords: list\n",
        "            removing_stopwords: bool\n",
        "\n",
        "        Returns:\n",
        "            cleaned review: str\n",
        "    \"\"\"\n",
        "    if type(review) != str:\n",
        "        review = str(review)  # Convert review to string if it's not already\n",
        "\n",
        "    if removing_stopwords == False:\n",
        "        reviews = cleaned_reviews(review)\n",
        "        return reviews\n",
        "    else:\n",
        "        reviews = cleaned_reviews(review)\n",
        "        reviews = stopword_removal(reviews,stopwords)\n",
        "        return reviews\n",
        "\n",
        "def cleaned_reviews(review):\n",
        "    \"\"\"This function takes a review string and removes unnecessary\n",
        "        punctuations, english characters, numbers.\n",
        "\n",
        "        Args:\n",
        "            review: str\n",
        "\n",
        "        Returns:\n",
        "            cleaned review: str\n",
        "    \"\"\"\n",
        "    review = review.replace('\\n', '') #removing new line\n",
        "    review = re.sub('[^\\u0980-\\u09FF]',' ',str(review)) #removing unnecessary punctuation\n",
        "    return review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuz46NjJ9g-n"
      },
      "source": [
        "## Remove Low Length Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BkTVnZo9g-o",
        "outputId": "59c84d6e-c27f-45e8-9835-ec049c9d9c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Cleaning: \n",
            "Removed 177 Small Reviews \n",
            "Total Reviews: 5823 \n",
            "Total Cyberbullying Reviews: 1686 \n",
            "Total Religious_Hatred Reviews: 673 \n",
            "Total Gender_Discrimination Reviews: 678 \n",
            "Total Sarcasm Reviews: 1518 \n",
            "Total Political Reviews: 776 \n",
            "Total Racism Reviews: 668\n"
          ]
        }
      ],
      "source": [
        "# Length of each Reveiws\n",
        "data['length'] = data['text'].astype(str).apply(lambda x: len(x.split()))\n",
        "# Remove the reviews with least words\n",
        "dataset = data.loc[data.length > 2]\n",
        "dataset = dataset.reset_index(drop=True)\n",
        "print(\n",
        "    \"After Cleaning:\",\"\\nRemoved {} Small Reviews\".format(len(data) - len(dataset)),\n",
        "    \"\\nTotal Reviews:\",len(dataset),\n",
        "    \"\\nTotal Cyberbullying Reviews:\",len(data[data.labels == 'Cyberbullying']),\n",
        "    \"\\nTotal Religious_Hatred Reviews:\",len(data[data.labels == 'Religious_Hatred']),\n",
        "    \"\\nTotal Gender_Discrimination Reviews:\",len(data[data.labels == 'Gender_Discrimination']),\n",
        "    \"\\nTotal Sarcasm Reviews:\",len(data[data.labels == 'Sarcasm']),\n",
        "    \"\\nTotal Political Reviews:\",len(data[data.labels == 'Political']),\n",
        "    \"\\nTotal Racism Reviews:\",len(data[data.labels == 'Racism'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "collapsed": true,
        "id": "9lUNYGWJ9g-p"
      },
      "outputs": [],
      "source": [
        "dataset[['text','labels']].to_csv('/content/Traning.xlsx - Sheet1 (6).csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwOcVu3O9g-p"
      },
      "source": [
        "### Save the cleaned data  and stopwords into a pickle file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('clean_rr_reviews.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "t3uXlMssILz7",
        "outputId": "5ded417c-0270-440e-e0c8-4817bf376e12"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'clean_rr_reviews.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-beeabc0b3468>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clean_rr_reviews.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'clean_rr_reviews.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TG3R63Ba9g-q"
      },
      "outputs": [],
      "source": [
        "# open a file, where you ant to store the data\n",
        "file = open('rr_review_data.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(data, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jxaPk-Nz9g-q"
      },
      "outputs": [],
      "source": [
        "# load the save file\n",
        "data = open('rr_review_data.pkl','rb')\n",
        "data = pickle.load(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JZ3ANKTE9g-r"
      },
      "outputs": [],
      "source": [
        "# Stopwords pickle\n",
        "stp = open(stopwords_list,'r', encoding='utf-8').read().split()\n",
        "# open a file, where you ant to store the data\n",
        "file = open('rr_stopwords.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(stp, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8whGttu9g-s"
      },
      "outputs": [],
      "source": [
        "stp = open('rr_stopwords.pkl','rb')\n",
        "stp = pickle.load(stp)\n",
        "len(stp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuYnTu-y9g-s"
      },
      "source": [
        "##### Processing of a sample review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XddygoL99g-s"
      },
      "outputs": [],
      "source": [
        "tweet = 'খাবার ভাল ছিল, তাছাড়া পরিবেশটা ও চমৎকার ।। ।।!!!!'\n",
        "process_reviews(review = tweet, stopwords =stopwords_list,removing_stopwords=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVkzxdnX9g-t"
      },
      "source": [
        "## Dataset Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5ni79gp9g-t"
      },
      "outputs": [],
      "source": [
        "from utils import data_summary\n",
        "documents,words,u_words,class_names = data_summary(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp1Gi21p9g-t"
      },
      "source": [
        "### Dataset Summary Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09slKtCy9g-t"
      },
      "outputs": [],
      "source": [
        "data_matrix = pd.DataFrame({'Total Documents':documents,\n",
        "                            'Total Words':words,\n",
        "                            'Unique Words':u_words,\n",
        "                            'Class Names':class_names})\n",
        "df = pd.melt(data_matrix, id_vars=\"Class Names\", var_name=\"Category\", value_name=\"Values\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE_a87nt9g-u"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "ax = plt.subplot()\n",
        "\n",
        "sns.barplot(data=df,x='Class Names', y='Values' ,hue='Category')\n",
        "ax.set_xlabel('Class Names')\n",
        "ax.set_title('Data Statistics')\n",
        "\n",
        "ax.xaxis.set_ticklabels(class_names, rotation=45);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITxNIGJE9g-u"
      },
      "source": [
        "### Review Length Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeNwEq5w9g-u"
      },
      "outputs": [],
      "source": [
        "# Calculate the Review of each of the Review\n",
        "dataset['ReviewLength'] = dataset.cleaned.apply(lambda x:len(x.split()))\n",
        "frequency = dict()\n",
        "for i in dataset.ReviewLength:\n",
        "    frequency[i] = frequency.get(i, 0)+1\n",
        "\n",
        "plt.bar(frequency.keys(), frequency.values(), color =\"b\")\n",
        "plt.xlim(1, 80)\n",
        "# in this notbook color is not working but it should work.\n",
        "plt.xlabel('Lenght of the Texts')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Length-Frequency Distribution')\n",
        "plt.show()\n",
        "print(f\"Maximum Length of a review: {max(dataset.ReviewLength)}\")\n",
        "print(f\"Minimum Length of a review: {min(dataset.ReviewLength)}\")\n",
        "print(f\"Average Length of a reviews: {round(np.mean(dataset.ReviewLength),0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9dleeUh9g-u"
      },
      "source": [
        "## Feature Extraction Using TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "D20neZv59g-v"
      },
      "outputs": [],
      "source": [
        "from utils import calc_unigram_tfidf,calc_bigram_tfidf,calc_trigram_tfidf,show_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DadUe4oe9g-v"
      },
      "outputs": [],
      "source": [
        "tweet = 'খাবার ভাল ছিল, তাছাড়া পরিবেশটা ও চমৎকার ।। ।।!!!!'\n",
        "cv,feature_vector = calc_trigram_tfidf(dataset.cleaned)\n",
        "print(\"Shape of TF-IDF Corpus =====>\",feature_vector.shape,'\\n')\n",
        "show_tfidf(cv,tweet)\n",
        "#first_vector = tfidf.transform([samp_review]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Hnjw2nBl9g-v"
      },
      "outputs": [],
      "source": [
        "#help(calc_unigram_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zkMgU-a9g-v"
      },
      "source": [
        "## ML Model Development Using Unigram Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04e7i8V79g-w"
      },
      "source": [
        "### Unigram Tf-idf Feature Extraction, Label Encoding and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TrxScK39g-w"
      },
      "outputs": [],
      "source": [
        "from utils import label_encoding,dataset_split\n",
        "from utils import calc_unigram_tfidf\n",
        "\n",
        "# calculate the Unigram Tf-idf feature\n",
        "cv,feature_vector = calc_unigram_tfidf(dataset.cleaned)\n",
        "# Encode the labels\n",
        "lables = label_encoding(dataset.Sentiment,False)\n",
        "# Split the Feature into train and test set\n",
        "X_train,X_test,y_train,y_test = dataset_split(feature_space=feature_vector,sentiment=lables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odGhq_Oo9g-x"
      },
      "source": [
        "### Model Defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qYREqz-X9g-x"
      },
      "outputs": [],
      "source": [
        "from utils import model_performace,ml_models_for_unigram_tfidf\n",
        "\n",
        "## classifiers defination\n",
        "ml_models,model_names = ml_models_for_unigram_tfidf()\n",
        "\n",
        "# call model accuracy function and save the metrices into a dictionary\n",
        "accuracy = {f'{model_names[i]}':model_performace(model,X_train,X_test,y_train,y_test) for i,model in enumerate(ml_models)}\n",
        "# Save the performance parameter into json file\n",
        "with open('ml_performance_unigram.json', 'w') as f:\n",
        "    json.dump(accuracy, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KiA-Udw9g-x"
      },
      "source": [
        "### Performance Table  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5-zGKXT9g_G"
      },
      "outputs": [],
      "source": [
        "from utils import performance_table\n",
        "\n",
        "# Load the json file\n",
        "accuracy = json.load(open('ml_performance_unigram.json'))\n",
        "table = performance_table(accuracy)\n",
        "table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HF1l6CE9g_N"
      },
      "outputs": [],
      "source": [
        "print(f\"Highest Accuracy achieved by {table.Accuracy.idxmax(axis = 0)} at = {max(table.Accuracy)}\")\n",
        "print(f\"Highest F1-Score achieved by {table['F1 Score'].idxmax(axis = 0)} at = {max(table['F1 Score'] )}\")\n",
        "print(f\"Highest Precision Score achieved by {table['Precision'].idxmax(axis = 0)} at = {max(table['Precision'] )}\")\n",
        "print(f\"Highest Recall Score achieved by {table['Recall'].idxmax(axis = 0)} at = {max(table['Recall'] )}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8znSeMl69g_O"
      },
      "source": [
        "### ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5Voq6SR9g_O"
      },
      "outputs": [],
      "source": [
        "from utils import plot_roc_curve,ml_models_for_unigram_tfidf\n",
        "## classifiers defination\n",
        "gram_models = ml_models_for_unigram_tfidf()\n",
        "\n",
        "plot_roc_curve(gram_models,X_train,X_test,y_train,y_test,'Unigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xQz8Y5I9g_b"
      },
      "source": [
        "### Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn5MV6HG9g_b"
      },
      "outputs": [],
      "source": [
        "from utils import plot_PR_curve,ml_models_for_unigram_tfidf\n",
        "\n",
        "gram_models = ml_models_for_unigram_tfidf()\n",
        "\n",
        "plot_PR_curve(gram_models,X_train,X_test,y_train,y_test,'Unigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ENh1OD9g_c"
      },
      "source": [
        "## Model Development Using Bigram Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAAHHHx29g_c"
      },
      "source": [
        "### Bi-gram Tf-idf Feature Extraction, Label Encoding and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPfiOarE9g_c"
      },
      "outputs": [],
      "source": [
        "from utils import label_encoding,dataset_split\n",
        "from utils import calc_bigram_tfidf\n",
        "\n",
        "# calculate the Bigram Tf-idf feature\n",
        "cv,feature_vector = calc_bigram_tfidf(dataset.cleaned)\n",
        "# Encode the labels\n",
        "lables = label_encoding(dataset.Sentiment,False)\n",
        "# Split the Feature into train and test set\n",
        "X_train,X_test,y_train,y_test = dataset_split(feature_space=feature_vector,sentiment=lables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dwsczjf9g_c"
      },
      "source": [
        "### Model Defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kpYxSphs9g_c"
      },
      "outputs": [],
      "source": [
        "from utils import model_performace,ml_models_for_bigram_tfidf\n",
        "\n",
        "# Classifiers Defination\n",
        "ml_models,model_names = ml_models_for_bigram_tfidf()\n",
        "\n",
        "# call model accuracy function and save the metrices into a dictionary\n",
        "accuracy = {f'{model_names[i]}':model_performace(model,X_train,X_test,y_train,y_test) for i,model in enumerate(ml_models)}\n",
        "# Save the performance parameter into json file\n",
        "with open('ml_performance_bigram.json', 'w') as f:\n",
        "    json.dump(accuracy, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhlTUsOS9g_d"
      },
      "source": [
        "### Performance Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtJ8mB6O9g_d"
      },
      "outputs": [],
      "source": [
        "from utils import performance_table\n",
        "\n",
        "# Load the json file\n",
        "accuracy = json.load(open('ml_performance_bigram.json'))\n",
        "table = performance_table(accuracy)\n",
        "table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SthN61a9g_d"
      },
      "outputs": [],
      "source": [
        "print(f\"Highest Accuracy achieved by {table.Accuracy.idxmax(axis = 0)} at = {max(table.Accuracy)}\")\n",
        "print(f\"Highest F1-Score achieved by {table['F1 Score'].idxmax(axis = 0)} at = {max(table['F1 Score'] )}\")\n",
        "print(f\"Highest Precision Score achieved by {table['Precision'].idxmax(axis = 0)} at = {max(table['Precision'] )}\")\n",
        "print(f\"Highest Recall Score achieved by {table['Recall'].idxmax(axis = 0)} at = {max(table['Recall'] )}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCzdy91W9g_d"
      },
      "source": [
        "### ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXS6eh279g_e"
      },
      "outputs": [],
      "source": [
        "from utils import plot_roc_curve,ml_models_for_bigram_tfidf\n",
        "## classifiers defination\n",
        "gram_models = ml_models_for_bigram_tfidf()\n",
        "\n",
        "plot_roc_curve(gram_models,X_train,X_test,y_train,y_test,'Bigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_e1O9xK9g_e"
      },
      "source": [
        "## Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utj2me0P9g_e"
      },
      "outputs": [],
      "source": [
        "from utils import plot_PR_curve,ml_models_for_bigram_tfidf\n",
        "## classifiers defination\n",
        "gram_models = ml_models_for_bigram_tfidf()\n",
        "\n",
        "plot_PR_curve(gram_models,X_train,X_test,y_train,y_test,'Bigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdtF9vMl9g_e"
      },
      "source": [
        "## Model Development Using Tri-gram Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Is2eaVk9g_e"
      },
      "source": [
        "### Tri-gram Tf-idf Feature Extraction, Label Encoding and Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WE_OJHy9g_f"
      },
      "outputs": [],
      "source": [
        "from utils import label_encoding,dataset_split\n",
        "from utils import calc_trigram_tfidf\n",
        "\n",
        "# calculate the Tri-gram Tf-idf feature\n",
        "cv,feature_vector = calc_trigram_tfidf(dataset.cleaned)\n",
        "# Encode the labels\n",
        "lables = label_encoding(dataset.Sentiment,False)\n",
        "# Split the Feature into train and test set\n",
        "X_train,X_test,y_train,y_test = dataset_split(feature_space=feature_vector,sentiment=lables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEr0mOxs9g_f"
      },
      "source": [
        "### Model Defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xOhmCBTM9g_f"
      },
      "outputs": [],
      "source": [
        "from utils import model_performace,ml_models_for_trigram_tfidf\n",
        "\n",
        "\n",
        "# Classifiers Defination\n",
        "ml_models,model_names = ml_models_for_trigram_tfidf()\n",
        "\n",
        "# call model accuracy function and save the metrices into a dictionary\n",
        "accuracy = {f'{model_names[i]}':model_performace(model,X_train,X_test,y_train,y_test) for i,model in enumerate(ml_models)}\n",
        "# Save the performance parameter into json file\n",
        "with open('ml_performance_trigram.json', 'w') as f:\n",
        "    json.dump(accuracy, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5cjPE_L9g_f"
      },
      "source": [
        "### Performance Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGHTRtyk9g_f"
      },
      "outputs": [],
      "source": [
        "from utils import performance_table\n",
        "\n",
        "# Load the json file\n",
        "accuracy = json.load(open('ml_performance_trigram.json'))\n",
        "table = performance_table(accuracy)\n",
        "table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r9zvjZH9g_f"
      },
      "outputs": [],
      "source": [
        "print(f\"Highest Accuracy achieved by {table.Accuracy.idxmax(axis = 0)} at = {max(table.Accuracy)}\")\n",
        "print(f\"Highest F1-Score achieved by {table['F1 Score'].idxmax(axis = 0)} at = {max(table['F1 Score'] )}\")\n",
        "print(f\"Highest Precision Score achieved by {table['Precision'].idxmax(axis = 0)} at = {max(table['Precision'] )}\")\n",
        "print(f\"Highest Recall Score achieved by {table['Recall'].idxmax(axis = 0)} at = {max(table['Recall'] )}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfQLzmZV9g_g"
      },
      "source": [
        "### ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2ftDO0M9g_g"
      },
      "outputs": [],
      "source": [
        "from utils import plot_roc_curve,ml_models_for_trigram_tfidf\n",
        "## classifiers defination\n",
        "gram_models = ml_models_for_trigram_tfidf()\n",
        "\n",
        "plot_roc_curve(gram_models,X_train,X_test,y_train,y_test,'Trigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdU7kh6Q9g_g"
      },
      "source": [
        "### Precision-Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "978s87r_9g_g"
      },
      "outputs": [],
      "source": [
        "from utils import plot_PR_curve,ml_models_for_trigram_tfidf\n",
        "## classifiers defination\n",
        "gram_models = ml_models_for_trigram_tfidf()\n",
        "\n",
        "plot_PR_curve(gram_models,X_train,X_test,y_train,y_test,'Trigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG1PYKjz9g_h"
      },
      "source": [
        "## Final Model\n",
        "\n",
        "- Selected Feature: Trigram\n",
        "- Selected Model : Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8SiW6Qr9g_h"
      },
      "outputs": [],
      "source": [
        "from utils import label_encoding,dataset_split\n",
        "from utils import calc_unigram_tfidf\n",
        "\n",
        "# calculate the Tri-gram Tf-idf feature\n",
        "cv,feature_vector = calc_trigram_tfidf(dataset.cleaned)\n",
        "# Encode the labels\n",
        "lables = label_encoding(dataset.Sentiment,False)\n",
        "# Split the Feature into train and test set\n",
        "X_train,X_test,y_train,y_test = dataset_split(feature_space=feature_vector,sentiment=lables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8T0mQ_eG9g_h"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "sgd_model = SGDClassifier(loss ='log',penalty='l2', max_iter=5)\n",
        "sgd_model.fit(X_train,y_train)\n",
        "y_pred = mnb_model.predict(X_test)\n",
        "accuracy_score(y_true=y_test,y_pred=y_pred)*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrm96hVR9g_i"
      },
      "source": [
        "### Saved the model for reuse again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sIxar2gO9g_i"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# open a file, where you ant to store the data\n",
        "file = open('rr_review_sgd.pkl', 'wb')\n",
        "\n",
        "# dump information to that file\n",
        "pickle.dump(sgd_model, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sKlZ7ya89g_i"
      },
      "outputs": [],
      "source": [
        "model = open('rr_review_sgd.pkl','rb')\n",
        "sgd = pickle.load(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya7LkVkN9g_i"
      },
      "outputs": [],
      "source": [
        "y_pred = sgd.predict(X_test)\n",
        "accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_JnVXL19g_j"
      },
      "source": [
        "## Check a Review Sentiment using our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtK3zyr79g_j"
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "model = open('rr_review_sgd.pkl','rb')\n",
        "sgd = pickle.load(model)\n",
        "######\n",
        "#review = 'aaaasd asd asdasd asd'\n",
        "review = 'খাবার ভাল ছিল , তাছাড়া পরিবেশটা ও চমৎকার ।। ।।!!!!'\n",
        "# Process the reviews\n",
        "processed_review = process_reviews(review,stopwords = stopwords_list,removing_stopwords = True)\n",
        "if (len(processed_review))>0:\n",
        "    # calculate the Unigram Tf-idf feature\n",
        "    cv,feature_vector = calc_trigram_tfidf(dataset.cleaned)\n",
        "    feature = cv.transform([processed_review]).toarray()\n",
        "\n",
        "    sentiment = sgd.predict(feature)\n",
        "    score = round(max(sgd.predict_proba(feature).reshape(-1)),2)*100\n",
        "\n",
        "    if (sentiment ==0):\n",
        "        print(f\"It is a Negative Review and the probability is {score}%\")\n",
        "    else:\n",
        "        print(f\"It is a Positive Review and the probability is {score}%\")\n",
        "else:\n",
        "    print(\"This review doesn't contains any bengali Words, thus cannot predict the Sentiment.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3W_qYlL9g_k"
      },
      "outputs": [],
      "source": [
        "review = 'aaaasd asd asdasd asd'\n",
        "# Process the reviews\n",
        "processed_review = process_reviews(review,stopwords = stopwords_list,removing_stopwords = True)\n",
        "len(processed_review)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}